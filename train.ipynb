{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yolov5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Person and Car Detection\n",
        "\n",
        "This code illustrates the transfer learning apporach for person and car detection on given data.\n",
        "\n",
        "\n",
        "\n",
        "1.   We will be using YOLOv5 model for our training. Alternatively we can use any detector model frameworks such as yolov3-v4, FasterRCNN or EfficientDets. But here for ease of training I've showcased the training with this model.\n",
        "2.   Our aim is to fine-tune a model on the given 2239 images. We'll initialize weights using pre-trained coco models since coco model is alredy trained on person,car images, it'll be easier to increase accuracy.\n",
        "3. This example is showed on Google Colab.\n",
        "\n"
      ],
      "metadata": {
        "id": "oATXzRSbDotv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQkLPel9CkEt",
        "outputId": "90a94a44-b6b6-457b-9796-b65844bb4c66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 12125, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 12125 (delta 0), reused 0 (delta 0), pack-reused 12122\u001b[K\n",
            "Receiving objects: 100% (12125/12125), 11.93 MiB | 25.89 MiB/s, done.\n",
            "Resolving deltas: 100% (8395/8395), done.\n"
          ]
        }
      ],
      "source": [
        "#clone the yolov5 repo.\n",
        "!git clone https://github.com/ultralytics/yolov5.git\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mounting google drive to upload the data.\n",
        "from google.colab import drive\n",
        "# drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEvpbNMyHR26",
        "outputId": "d97d51ff-b316-4b58-c65f-30d2dff80896"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before uploading the data, I've already segregated the data in train/ and val/ folders. \n",
        "\n",
        "Training Images: 2000 \n",
        "Validation Images: 239 \n",
        "\n",
        "Now, the task is to convert the JSON annotations to YOLO format for training."
      ],
      "metadata": {
        "id": "sDHj9nx0EhFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "FXf_1HucHYnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This cell, converts the input COCO json to YOLO Txt format with the\n",
        "#required format of \"category_id, x, y, w , h\"\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "\n",
        "\n",
        "def make_folders(path=\"output\"):\n",
        "    if os.path.exists(path):\n",
        "        shutil.rmtree(path)\n",
        "    os.makedirs(path)\n",
        "    return path\n",
        "\n",
        "def convert_bbox_coco2yolo(img_width, img_height, bbox):\n",
        "    \"\"\"\n",
        "    Convert bounding box from COCO  format to YOLO format\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    img_width : int\n",
        "        width of image\n",
        "    img_height : int\n",
        "        height of image\n",
        "    bbox : list[int]\n",
        "        bounding box annotation in COCO format: \n",
        "        [top left x position, top left y position, width, height]\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list[float]\n",
        "        bounding box annotation in YOLO format: \n",
        "        [x_center_rel, y_center_rel, width_rel, height_rel]\n",
        "    \"\"\"\n",
        "    \n",
        "    # YOLO bounding box format: [x_center, y_center, width, height]\n",
        "    # (float values relative to width and height of image)\n",
        "    x_tl, y_tl, w, h = bbox\n",
        "\n",
        "    dw = 1.0 / img_width\n",
        "    dh = 1.0 / img_height\n",
        "\n",
        "    x_center = x_tl + w / 2.0\n",
        "    y_center = y_tl + h / 2.0\n",
        "\n",
        "    x = x_center * dw\n",
        "    y = y_center * dh\n",
        "    w = w * dw\n",
        "    h = h * dh\n",
        "\n",
        "    return [x, y, w, h]\n",
        "  \n",
        "def convert_coco_json_to_yolo_txt(output_path, json_file):\n",
        "\n",
        "    path = make_folders(output_path)\n",
        "\n",
        "    with open(json_file) as f:\n",
        "        json_data = json.load(f)\n",
        "\n",
        "    # write _darknet.labels, which holds names of all classes (one class per line)\n",
        "    label_file = os.path.join(output_path, \"_darknet.labels\")\n",
        "    with open(label_file, \"w\") as f:\n",
        "        for category in tqdm(json_data[\"categories\"], desc=\"Categories\"):\n",
        "            category_name = category[\"name\"]\n",
        "            f.write(f\"{category_name}\\n\")\n",
        "\n",
        "    for image in tqdm(json_data[\"images\"], desc=\"Annotation txt for each iamge\"):\n",
        "        img_id = image[\"id\"]\n",
        "        img_name = image[\"file_name\"]\n",
        "        img_width = image[\"width\"]\n",
        "        img_height = image[\"height\"]\n",
        "\n",
        "        anno_in_image = [anno for anno in json_data[\"annotations\"] if anno[\"image_id\"] == img_id]\n",
        "        anno_txt = os.path.join(output_path, img_name.split(\".\")[0] + \".txt\")\n",
        "        with open(anno_txt, \"w\") as f:\n",
        "            for anno in anno_in_image:\n",
        "                category = anno[\"category_id\"] -1 \n",
        "                bbox_COCO = anno[\"bbox\"]\n",
        "                x, y, w, h = convert_bbox_coco2yolo(img_width, img_height, bbox_COCO)\n",
        "                f.write(f\"{category} {x:.6f} {y:.6f} {w:.6f} {h:.6f}\\n\")\n",
        "\n",
        "    print(\"Converting COCO Json to YOLO txt finished!\")"
      ],
      "metadata": {
        "id": "6LN22Qo7B4pN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling the function to convert all the data into TXT"
      ],
      "metadata": {
        "id": "p3EChcxHF531"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convert_coco_json_to_yolo_txt(\"/content/drive/MyDrive/person_car/data_train/all_labels/\", \"/content/drive/MyDrive/person_car/data_train/annotation_json/bbox-annotations.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mh31VpLFQJw",
        "outputId": "09fffdf1-2922-49c2-a114-2bde9bc78542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Categories: 100%|██████████| 2/2 [00:00<00:00, 11602.50it/s]\n",
            "Annotation txt for each iamge: 100%|██████████| 2239/2239 [00:28<00:00, 79.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting COCO Json to YOLO txt finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#moving labels of train/val into their respective folders.\n",
        "def move_files(source_img_dir, source_label_dir, target_label_dir):\n",
        "  for each_img in os.listdir(source_img_dir):\n",
        "    # print(each_img)\n",
        "    txt_name = each_img.split(\".\")[0]+ \".txt\"\n",
        "    source_file = os.path.join(source_label_dir, txt_name)\n",
        "    target_file = os.path.join(target_label_dir, txt_name)\n",
        "    print(txt_name)\n",
        "    shutil.copy(source_file, target_file)\n"
      ],
      "metadata": {
        "id": "uPgxiNhPCeSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below chunk of the code moves training labels and validation labels to their respective folders according to image names."
      ],
      "metadata": {
        "id": "HHGGhzr8GABJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ALL_LABELS = \"/content/drive/MyDrive/person_car/data_train/all_labels/\"\n",
        "TRAIN_DIR = \"/content/drive/MyDrive/person_car/data_train/train\"\n",
        "VAL_DIR = \"/content/drive/MyDrive/person_car/data_train/val\"\n",
        "\n",
        "train_imgs = os.path.join(TRAIN_DIR, \"images\")\n",
        "val_imgs = os.path.join(VAL_DIR, \"images\")  \n",
        "\n",
        "train_label_dir = os.path.join(TRAIN_DIR, \"labels\")\n",
        "val_label_dir = os.path.join(VAL_DIR, \"labels\")\n",
        "if not os.path.exists(train_label_dir):\n",
        "  os.mkdir(os.path.join(TRAIN_DIR, \"labels\"))\n",
        "if not os.path.exists(val_label_dir):\n",
        "  os.mkdir(os.path.join(VAL_DIR, \"labels\"))\n",
        "\n",
        "\n",
        "\n",
        "move_files(train_imgs, ALL_LABELS, train_label_dir)\n",
        "move_files(val_imgs, ALL_LABELS, val_label_dir)\n",
        "\n"
      ],
      "metadata": {
        "id": "o4DCjC7DHGlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for training, we need to create a .YAML file to configure the train/val paths. Here, created a custom.yaml with required information."
      ],
      "metadata": {
        "id": "bgIsH_foGKQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a yaml for training \n",
        "import yaml\n",
        "data = dict(\n",
        "    names = ['person', 'car'],\n",
        "    nc = 2,\n",
        "    path= \"/content/drive/MyDrive/person_car/data_train/\",\n",
        "    train=\"/content/drive/MyDrive/person_car/data_train/train/images\",\n",
        "    val=\"/content/drive/MyDrive/person_car/data_train/val/images\"\n",
        "\n",
        ")\n",
        "\n",
        "with open('/content/yolov5/custom.yaml', 'w') as outfile:\n",
        "    yaml.dump(data, outfile, default_flow_style=False)"
      ],
      "metadata": {
        "id": "MlyABlQvSBN6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start Training\n",
        "\n",
        "We will train our model with input_size 416x416 for 10 epochs.\n",
        "\n",
        "*   We're using pretrained model `yolov5m.pt` to initialize. If we want deeper network with high accuracy we can use higher models from here: https://github.com/ultralytics/yolov5#pretrained-checkpoints\n",
        "\n",
        "*   We can also train on higher resolution i.e 608 to achieve better performance.\n"
      ],
      "metadata": {
        "id": "qOtagJBAGYaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/yolov5/train.py --img 416 --batch 8 --epochs 10 --data /content/yolov5/custom.yaml --weights yolov5m.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TyzjlHbTdVk",
        "outputId": "9be14b65-530c-45b8-cca7-23ce449f36b4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5m.pt, cfg=, data=/content/yolov5/custom.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=8, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=yolov5/runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v6.1-235-g632559b Python-3.7.13 torch-1.11.0+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolov5/runs/train', view at http://localhost:6006/\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 28.4MB/s]\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5m.pt to yolov5m.pt...\n",
            "100% 40.8M/40.8M [00:02<00:00, 14.9MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n",
            "  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n",
            "  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n",
            "  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n",
            "  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n",
            "  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n",
            "  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n",
            "  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n",
            "  8                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n",
            "  9                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n",
            " 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n",
            " 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n",
            " 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n",
            " 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  2   4134912  models.common.C3                        [768, 768, 2, False]          \n",
            " 24      [17, 20, 23]  1     28287  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n",
            "Model summary: 369 layers, 20875359 parameters, 20875359 gradients\n",
            "\n",
            "Transferred 475/481 items from yolov5m.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 79 weight (no decay), 82 weight, 82 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/drive/MyDrive/person_car/data_train/train/labels.cache' images and labels... 2000 found, 0 missing, 0 empty, 0 corrupt: 100% 2000/2000 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/drive/MyDrive/person_car/data_train/val/labels.cache' images and labels... 239 found, 0 missing, 0 empty, 0 corrupt: 100% 239/239 [00:00<?, ?it/s]\n",
            "Plotting labels to yolov5/runs/train/exp2/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.53 anchors/target, 0.994 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1myolov5/runs/train/exp2\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       0/9     1.52G   0.08692   0.05832   0.02017       140       416: 100% 250/250 [09:47<00:00,  2.35s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 15/15 [00:05<00:00,  2.86it/s]\n",
            "                 all        239       1609      0.525      0.502      0.474      0.214\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       1/9     1.95G   0.06729   0.05311  0.009442       101       416: 100% 250/250 [02:03<00:00,  2.03it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 15/15 [00:05<00:00,  2.74it/s]\n",
            "                 all        239       1609      0.486      0.505      0.477      0.228\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       2/9     1.95G   0.06236   0.05294  0.008625       162       416: 100% 250/250 [02:03<00:00,  2.02it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 15/15 [00:05<00:00,  2.74it/s]\n",
            "                 all        239       1609      0.621      0.606      0.603       0.31\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       3/9     1.95G   0.05671   0.05191  0.007904       166       416: 100% 250/250 [02:03<00:00,  2.03it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 15/15 [00:05<00:00,  2.85it/s]\n",
            "                 all        239       1609       0.75      0.641      0.686      0.359\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       4/9     1.95G   0.05277   0.05231  0.007489       103       416: 100% 250/250 [02:03<00:00,  2.03it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 15/15 [00:05<00:00,  2.79it/s]\n",
            "                 all        239       1609      0.758      0.638      0.699      0.387\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       5/9     1.95G    0.0495   0.05245  0.007129        86       416: 100% 250/250 [02:02<00:00,  2.03it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 15/15 [00:05<00:00,  2.76it/s]\n",
            "                 all        239       1609       0.76      0.621      0.694      0.387\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       6/9     1.95G   0.04791   0.05121  0.006715        91       416: 100% 250/250 [02:02<00:00,  2.04it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 15/15 [00:05<00:00,  2.68it/s]\n",
            "                 all        239       1609      0.772      0.638      0.703      0.403\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       7/9     1.95G   0.04672   0.05032  0.006418       102       416: 100% 250/250 [02:02<00:00,  2.04it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 15/15 [00:05<00:00,  2.94it/s]\n",
            "                 all        239       1609      0.753      0.655      0.706       0.41\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       8/9     1.95G   0.04533   0.05075  0.006105        77       416: 100% 250/250 [02:02<00:00,  2.05it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 15/15 [00:05<00:00,  2.75it/s]\n",
            "                 all        239       1609      0.772      0.654      0.719       0.42\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       9/9     1.95G   0.04466    0.0508  0.005777       134       416: 100% 250/250 [02:03<00:00,  2.02it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 15/15 [00:05<00:00,  2.76it/s]\n",
            "                 all        239       1609      0.776      0.648      0.717      0.428\n",
            "\n",
            "10 epochs completed in 0.491 hours.\n",
            "Optimizer stripped from yolov5/runs/train/exp2/weights/last.pt, 42.1MB\n",
            "Optimizer stripped from yolov5/runs/train/exp2/weights/best.pt, 42.1MB\n",
            "\n",
            "Validating yolov5/runs/train/exp2/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 290 layers, 20856975 parameters, 0 gradients\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 15/15 [00:06<00:00,  2.39it/s]\n",
            "                 all        239       1609      0.776      0.649      0.717      0.428\n",
            "              person        239       1003      0.737      0.632      0.689      0.365\n",
            "                 car        239        606      0.814      0.666      0.744       0.49\n",
            "Results saved to \u001b[1myolov5/runs/train/exp2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CPO_cu4hT-vX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}